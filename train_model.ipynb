{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING  user config directory is not writeable, defaulting to '/tmp/Ultralytics'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from tools.yolo_utils import read_yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Tasks Supported by YOLOv8:\n",
    "\n",
    "1. **Detection**:\n",
    "   - Objective: Detect objects in an image or video frame and draw bounding boxes around them.\n",
    "   - Use Case: Object detection in various applications such as surveillance, autonomous vehicles, and robotics.\n",
    "\n",
    "2. **Segmentation**:\n",
    "   - Objective: Segment an image into different regions based on its content and assign labels to each region.\n",
    "   - Use Case: Image segmentation, medical imaging, and understanding object boundaries.\n",
    "\n",
    "3. **Classification**:\n",
    "   - Objective: Classify an image into different categories based on its content.\n",
    "   - Use Case: Image classification in various applications such as object recognition, content filtering, and visual search.\n",
    "\n",
    "4. **Pose/Keypoint Detection**:\n",
    "   - Objective: Detect specific points (keypoints) in an image or video frame for tracking movement or pose estimation.\n",
    "   - Use Case: Human pose estimation, motion tracking, and gesture recognition.\n",
    "\n",
    "Note: YOLOv8 utilizes different architectures (e.g., U-Net, EfficientNet) to perform segmentation, classification, and pose/keypoint detection, ensuring accuracy and speed in these tasks.\n",
    "### Models Supported by YOLOv8:\n",
    "The following models are available in YOLOv8 and are suitable starting points for various tasks:\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "| Model Type   | Pre-trained Weights                            | Task                |\n",
    "|--------------|------------------------------------------------|---------------------|\n",
    "| YOLOv8       | yolov8n.pt, yolov8s.pt, yolov8m.pt, yolov8l.pt, yolov8x.pt       | Detection           |\n",
    "| YOLOv8-seg   | yolov8n-seg.pt, yolov8s-seg.pt, yolov8m-seg.pt, yolov8l-seg.pt, yolov8x-seg.pt   | Instance Segmentation |\n",
    "| YOLOv8-pose  | yolov8n-pose.pt, yolov8s-pose.pt, yolov8m-pose.pt, yolov8l-pose.pt, yolov8x-pose.pt, yolov8x-pose-p6 | Pose/Keypoints      |\n",
    "| YOLOv8-cls   | yolov8n-cls.pt, yolov8s-cls.pt, yolov8m-cls.pt, yolov8l-cls.pt, yolov8x-cls.pt     | Classification      |\n",
    "\n",
    "</div>\n",
    "\n",
    "## Model and project settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model    = 'yolov8l.pt'                                    # Initial weights path. Can aslo be a .yaml defining a model, e.g. yolov8m.yaml\n",
    "task     = 'detect'                                        # 'detect', 'classify', 'segment'\n",
    "project  = 'yolov8/'                                       # Project folder\n",
    "name     = 'schenk8l'                                      # Model name\n",
    "exist_ok = True                                            # Overwrite existing project folder\n",
    "seed     = 0                                               # Seed for training\n",
    "resume   = False                                           # Resume training from last.pt from project + name\n",
    "verbose  = False                                           # Print detailed results\n",
    "plots    = False                                           # Plot training results\n",
    "project_dir = os.path.join(os.getcwd(), project, name)     # Project directory\n",
    "device   = '0'                                             # CUDA device, i.e. '0' or '0,1,2,3' or 'cpu'\n",
    "\n",
    "project_settings = {\n",
    "    'name': name,\n",
    "    'project': project,\n",
    "    'exist_ok': exist_ok,\n",
    "    'seed': seed,\n",
    "    'verbose': verbose,\n",
    "    'resume': resume,\n",
    "    'plots': plots,\n",
    "    'device': device,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Datasets\n",
    "These datasets may be used to train or pre-train a model in YOLOv8. The datasets are can also be found [here](https://docs.ultralytics.com/datasets/).\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Detection\n",
    "</summary>\n",
    "\n",
    "| Dataset       | Description                                                      |\n",
    "|---------------|------------------------------------------------------------------|\n",
    "| Argoverse     | 3D tracking and motion forecasting data from urban environments. |\n",
    "| COCO          | Large-scale dataset for object detection, segmentation, and captioning. |\n",
    "| COCO8         | Subset of COCO train and COCO val for quick tests.               |\n",
    "| Global Wheat 2020 | Dataset of wheat head images for object detection and localization. |\n",
    "| Objects365    | High-quality dataset for object detection with 365 categories.   |\n",
    "| SKU-110K      | Dense object detection dataset in retail environments.           |\n",
    "| VisDrone      | Dataset with object detection and multi-object tracking from drone-captured imagery. |\n",
    "| VOC           | Pascal Visual Object Classes dataset for object detection and segmentation. |\n",
    "| xView         | Dataset for object detection in overhead imagery.                |\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Instance Segmentation\n",
    "</summary>\n",
    "\n",
    "| Dataset       | Description                                                      |\n",
    "|---------------|------------------------------------------------------------------|\n",
    "| COCO          | Large-scale dataset for object detection, segmentation, and captioning. |\n",
    "| COCO8-seg     | Subset of COCO with segmentation annotations.                    |\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Pose Estimation\n",
    "</summary>\n",
    "\n",
    "| Dataset       | Description                                                      |\n",
    "|---------------|------------------------------------------------------------------|\n",
    "| COCO          | Large-scale dataset with human pose annotations.                  |\n",
    "| COCO8-pose    | Subset of COCO with human pose annotations.                       |\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Classification\n",
    "</summary>\n",
    "\n",
    "| Dataset       | Description                                                      |\n",
    "|---------------|------------------------------------------------------------------|\n",
    "| Caltech 101   | Dataset with images of 101 object categories for classification.  |\n",
    "| Caltech 256   | Extended version of Caltech 101 with 256 categories.             |\n",
    "| CIFAR-10      | Dataset of color images in 10 classes.                            |\n",
    "| CIFAR-100     | Extended version of CIFAR-10 with 100 categories.                 |\n",
    "| Fashion-MNIST | Dataset with grayscale images of fashion categories.              |\n",
    "| ImageNet      | Large-scale dataset for object detection and classification.      |\n",
    "| ImageNet-10   | Subset of ImageNet with 10 categories.                            |\n",
    "| Imagewoof     | Challenging subset of ImageNet with 10 dog breed categories.      |\n",
    "| Imagenette    | Smaller subset of ImageNet with 10 easily distinguishable classes.|\n",
    "| MNIST         | Dataset of grayscale images of handwritten digits.                |\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Multi-Object Tracking\n",
    "</summary>\n",
    "\n",
    "| Dataset       | Description                                                      |\n",
    "|---------------|------------------------------------------------------------------|\n",
    "| Argoverse     | 3D tracking and motion forecasting data from urban environments. |\n",
    "| VisDrone      | Dataset with object detection and multi-object tracking from drone-captured imagery. |\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Constructing a YOLO Custom Dataset\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Step-by-step\n",
    "</summary>\n",
    "\n",
    "1. Collect and Organize Your Data:\n",
    "   - Gather images relevant to the object(s) you want to detect.\n",
    "   - Create separate directories for images and annotation files.\n",
    "\n",
    "2. Annotate the Images:\n",
    "   - Use an annotation tool to mark the bounding boxes around objects of interest.\n",
    "   - Save annotations in a YOLO-compatible format.\n",
    "   - Text files with one row per object instance: ```<class_id> <x_center> <y_center> <width> <height>```\n",
    "   - This can be done using various tools such as [Roboflow](https://roboflow.com/) **online** or [YoloLabel](https://github.com/developer0hye/Yolo_Label) **offline**.\n",
    "\n",
    "3. Split the Dataset:\n",
    "   - Divide your dataset into training, validation, and testing sets.\n",
    "   - Ensure each set has a representative distribution of classes and object instances.\n",
    "\n",
    "4. Generate data.yaml file:\n",
    "   - Contains information about the dataset and paths to the training, validation, and testing sets.\n",
    "   - Class names are listed in the order of their IDs in ```names```.\n",
    "   - Number of classes is specified in the ```nc``` field.\n",
    "   - Example:\n",
    "   ```yaml\n",
    "      names:\n",
    "      - '0'\n",
    "      - '1'\n",
    "      - '2'\n",
    "      - '3'\n",
    "      - '4'\n",
    "      - '5'\n",
    "      - '6'\n",
    "      - '7'\n",
    "      - '8'\n",
    "      - '9'\n",
    "      nc: 10\n",
    "      path: /path/to/dataset/ # Path to dataset directory.\n",
    "      test: test              # Relative to path above.\n",
    "      train: train            \n",
    "      val: val               \n",
    "   ```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data settings\n",
    "```python\n",
    "data_path  = '/home/thjo/Datasets/BolidenDigits/data.yaml' # Path to data.yaml\n",
    "imgsz      = 416                                           # Image size.\n",
    "batch_size = -1                                           # Batch size, '-1' uses the largest batch size that fits on the GPS(s)     \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = '/home/thjo/Datasets/BolidenSchenk/data.yaml'      # Path to data.yaml\n",
    "imgsz      = 416                                           # Image size.\n",
    "batch      = 16                                            # Batch size, '-1' uses the largest batch size that fits on the GPU(s)\n",
    "fraction   = 1.0                                           # Fraction of dataset to use for training (0-1) Useful for debugging to validate code and convergence\n",
    "\n",
    "data_settings ={\n",
    "    'data': data,\n",
    "    'imgsz': imgsz,\n",
    "    'batch': batch,\n",
    "    'fraction': fraction\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training settings\n",
    "The following are a subset of the available training settings. For a complete list, see [here](https://docs.ultralytics.com/modes/train/#arguments).\n",
    "\n",
    "<details>\n",
    "<summary>lr0 - Initial Learning Rate</summary>\n",
    "The lr0 parameter represents the initial learning rate of the model. It determines the step size at the beginning of training, influencing how quickly the model learns from the data. A higher learning rate can lead to faster convergence, but it may also cause instability or overshooting. Conversely, a lower learning rate may result in slower convergence but can lead to more accurate and stable training.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>val - Validation during Training</summary>\n",
    "The val parameter is a boolean flag indicating whether to validate the model on the validation set during training. Validation allows monitoring the model's performance on unseen data and helps in detecting overfitting or underfitting. By evaluating the model's performance on the validation set, you can make informed decisions regarding model selection and hyperparameter tuning.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>epochs - Number of Epochs</summary>\n",
    "The epochs parameter determines the total number of times the model will iterate over the entire training dataset. An epoch represents a complete pass through the entire training data, and each epoch updates the model's parameters based on the optimization algorithm used. Choosing an appropriate number of epochs is important to balance training time and model convergence. Too few epochs may result in an undertrained model, while too many epochs may lead to overfitting.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>patience - Early Stopping</summary>\n",
    "The patience parameter refers to the number of epochs to wait before stopping training if there is no improvement in the validation metric. It enables early stopping, a technique used to prevent overfitting and improve efficiency. If the model's performance on the validation set does not improve for a specified number of epochs (defined by patience), training is stopped early to avoid wasting computational resources on a non-improving model.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>save_period - Model Saving</summary>\n",
    "The save_period parameter determines how frequently the model is saved during training. Setting a positive value for save_period means that the model will be saved every n epochs. This allows you to have checkpoints of the model at regular intervals during training. Alternatively, setting save_period to -1 disables automatic saving of the model. Manually saving the model can be done at any desired point in the code.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>device - Hardware Device</summary>\n",
    "The device parameter specifies the hardware device to use for training the model. If you have a compatible GPU, you can specify the CUDA device ID (e.g., '0') to utilize GPU acceleration, which can significantly speed up the training process. Alternatively, you can set device to 'cpu' for CPU training. Choosing the appropriate device depends on the availability of hardware resources and the size of the dataset.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>cache - Image Caching</summary>\n",
    "The cache parameter determines whether to cache images for faster training. Caching preprocessed images in memory can improve the training speed by reducing disk I/O and preprocessing overhead. However, caching can consume a significant amount of memory, so it is advisable to consider the available memory resources and the size of the dataset before enabling this option.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>workers - CPU Workers</summary>\n",
    "The workers parameter specifies the number of CPU workers to use for data loading during training. Increasing the number of workers enables parallel data loading, which can speed up the training process, especially when there are bottlenecks in data loading and preprocessing. However, the optimal number of workers depends on the available CPU resources and the complexity of data loading operations.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>optimizer - Optimization Algorithm</summary>\n",
    "The optimizer parameter determines the algorithm used to update the model's parameters during training. Options include SGD (Stochastic Gradient Descent), Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, or 'auto' to automatically select an optimizer based on the model architecture and problem. Each optimizer has its own set of hyperparameters that control the learning process. Choosing an appropriate optimizer and its hyperparameters is crucial for achieving good training performance.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>cos_lr - Cosine Learning Rate Scheduler</summary>\n",
    "The cos_lr parameter indicates whether to use a cosine learning rate scheduler. The cosine learning rate scheduler gradually reduces the learning rate during training. \n",
    "This technique is based on the cosine function and can help the model converge to a better optima. Using a cosine learning rate scheduler can potentially improve the model's performance and generalization.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr0          = 0.005                                      # Initial learning rate\n",
    "val          = False                                      # Validate on validation set during training\n",
    "epochs       = 100                                        # Number of epochs\n",
    "patience     = 25                                         # Stop training after this many epochs without improvement (early stopping)\n",
    "save_period  = -1                                         # Save model every n epochs, -1 to disable\n",
    "device       = '0'                                        # Device (cuda device id)\n",
    "cache        = False                                      # Cache images for faster training\n",
    "workers      = 8                                          # Number of CPU workers\n",
    "optimizer    = 'auto'                                     # Optimizer (SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto)\n",
    "cos_lr       = True                                       # Use cosine learning rate scheduler\n",
    "lrf          = 0.0001                                     # Final learning rate (for cosine scheduler)\n",
    "momentum     = 0.937                                      # SGD momentum/Adam beta1\n",
    "weight_decay = 0.0005                                     # optimizer weight decay\n",
    "warmup_epochs= 3                                          # Warmup epochs (fractions ok)\n",
    "warmup_momentum = 0.8                                     # Warmup initial momentum\n",
    "\n",
    "training_settings = {\n",
    "    'lr0': lr0,\n",
    "    'val': val,\n",
    "    'epochs': epochs,\n",
    "    'patience': patience,\n",
    "    'save_period': save_period,\n",
    "    'device': device,\n",
    "    'cache': cache,\n",
    "    'workers': workers,\n",
    "    'optimizer': optimizer,\n",
    "    'cos_lr': cos_lr,\n",
    "    'lrf': lrf,\n",
    "    'momentum': momentum,\n",
    "    'weight_decay': weight_decay,\n",
    "    'warmup_epochs': warmup_epochs,\n",
    "    'warmup_momentum': warmup_momentum\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations\n",
    "YoloV8 supports a wide range of augmentations. For a complete list, see [here](https://docs.ultralytics.com/usage/cfg/#augmentation).\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "\n",
    "| Parameter    | Value  | Description                                 |\n",
    "|--------------|--------|---------------------------------------------|\n",
    "| hsv_h        | 0.015  | Image HSV-Hue augmentation (fraction)        |\n",
    "| hsv_s        | 0.7    | Image HSV-Saturation augmentation (fraction) |\n",
    "| hsv_v        | 0.4    | Image HSV-Value augmentation (fraction)      |\n",
    "| degrees      | 0.0    | Image rotation (+/- deg)                     |\n",
    "| translate    | 0.1    | Image translation (+/- fraction)             |\n",
    "| scale        | 0.5    | Image scale (+/- gain)                       |\n",
    "| shear        | 0.0    | Image shear (+/- deg)                        |\n",
    "| perspective  | 0.0    | Image perspective (+/- fraction)             |\n",
    "| flipud       | 0.0    | Image flip up-down (probability)             |\n",
    "| fliplr       | 0.5    | Image flip left-right (probability)          |\n",
    "| mosaic       | 1.0    | Image mosaic (probability)                   |\n",
    "| mixup        | 0.0    | Image mixup (probability)                    |\n",
    "| copy_paste   | 0.0    | Segment copy-paste (probability)             |\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "### Standard Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = {\n",
    "    'hsv_h': 0.015,\n",
    "    'hsv_s': 0.7,\n",
    "    'hsv_v': 0.4,\n",
    "    'degrees': 10.0,\n",
    "    'translate': 0.1,\n",
    "    'scale': 0.5,\n",
    "    'shear': 0.0,\n",
    "    'perspective': 0.0,\n",
    "    'flipud': 0.0,\n",
    "    'fliplr': 0.0,\n",
    "    'mosaic': 1.0,\n",
    "    'mixup': 0.0,\n",
    "    'copy_paste': 0.0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    **project_settings,\n",
    "    **data_settings,\n",
    "    **training_settings,\n",
    "    **augmentations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.116 🚀 Python-3.9.16 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 7973MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8l.pt, data=/home/thjo/Datasets/BolidenSchenk/data.yaml, epochs=100, patience=25, batch=16, imgsz=416, save=True, save_period=-1, cache=False, device=0, workers=8, project=yolov8/, name=schenk8l, exist_ok=True, pretrained=False, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=False, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=False, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.005, lrf=0.0001, momentum=0.937, weight_decay=0.0005, warmup_epochs=3, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=10.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=yolov8/schenk8l\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to /tmp/Ultralytics/Arial.ttf...\n",
      "100%|██████████| 755k/755k [00:00<00:00, 8.37MB/s]\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5584342  ultralytics.nn.modules.head.Detect           [2, [256, 512, 512]]          \n",
      "Model summary: 365 layers, 43631382 parameters, 43631366 gradients, 165.4 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/thjo/Datasets/BolidenSchenk/train.cache... 2126 images, 26 backgrounds, 3 corrupt: 100%|██████████| 2126/2126 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/thjo/Datasets/BolidenSchenk/train/item_1346.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0017]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/thjo/Datasets/BolidenSchenk/train/item_2081.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0069]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/thjo/Datasets/BolidenSchenk/train/item_2318.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0017]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/thjo/Datasets/BolidenSchenk/val.cache... 248 images, 1 backgrounds, 0 corrupt: 100%|██████████| 248/248 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
      "Image sizes 416 train, 416 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1myolov8/schenk8l\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      1/100      4.77G     0.9029     0.9447      1.229         50        416: 100%|██████████| 133/133 [00:37<00:00,  3.57it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      2/100      4.79G     0.8614     0.7112      1.183         54        416: 100%|██████████| 133/133 [00:35<00:00,  3.78it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      3/100      4.82G     0.8319     0.6648      1.152         44        416: 100%|██████████| 133/133 [00:33<00:00,  3.93it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      4/100      4.79G     0.7592     0.6037      1.105         39        416: 100%|██████████| 133/133 [00:33<00:00,  3.96it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      5/100      4.82G     0.7287     0.5651      1.088         49        416: 100%|██████████| 133/133 [00:33<00:00,  4.02it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      6/100      4.81G     0.6665     0.4921      1.047         54        416: 100%|██████████| 133/133 [00:33<00:00,  3.96it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      7/100       4.8G     0.6649     0.4897      1.047         44        416: 100%|██████████| 133/133 [00:33<00:00,  3.98it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      8/100      4.79G     0.6233     0.4541      1.023         48        416: 100%|██████████| 133/133 [00:33<00:00,  3.94it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      9/100      4.82G     0.5968     0.4295      1.011         47        416: 100%|██████████| 133/133 [00:34<00:00,  3.85it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     10/100      4.81G     0.6072     0.4343      1.016         56        416: 100%|██████████| 133/133 [00:34<00:00,  3.82it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     11/100       4.8G     0.5795     0.4152      1.008         56        416: 100%|██████████| 133/133 [00:34<00:00,  3.90it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     12/100      4.79G     0.5699     0.4096      1.004         44        416: 100%|██████████| 133/133 [00:33<00:00,  3.99it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     13/100      4.82G     0.5572     0.3984     0.9871         57        416: 100%|██████████| 133/133 [00:33<00:00,  3.94it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     14/100      4.81G     0.5397     0.3888     0.9893         45        416: 100%|██████████| 133/133 [00:33<00:00,  4.01it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     15/100       4.8G     0.5543     0.3803     0.9943         50        416: 100%|██████████| 133/133 [00:34<00:00,  3.89it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     16/100      4.79G     0.5428     0.3667     0.9782         48        416: 100%|██████████| 133/133 [00:33<00:00,  3.95it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     17/100      4.82G     0.5096     0.3587      0.969         38        416: 100%|██████████| 133/133 [00:33<00:00,  3.98it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     18/100      4.81G     0.5124     0.3561     0.9725         54        416: 100%|██████████| 133/133 [00:34<00:00,  3.89it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     19/100       4.8G     0.5033     0.3465     0.9643         63        416: 100%|██████████| 133/133 [00:33<00:00,  3.93it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     20/100      4.79G     0.4998     0.3394      0.966         53        416: 100%|██████████| 133/133 [00:33<00:00,  4.01it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     21/100      4.82G     0.4942     0.3346     0.9618         53        416: 100%|██████████| 133/133 [00:30<00:00,  4.34it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     22/100      4.81G     0.4801     0.3269     0.9556         55        416: 100%|██████████| 133/133 [00:32<00:00,  4.11it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     23/100       4.8G      0.475     0.3272     0.9541         48        416: 100%|██████████| 133/133 [00:30<00:00,  4.32it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     24/100      4.79G     0.4746     0.3263     0.9558         52        416: 100%|██████████| 133/133 [00:32<00:00,  4.12it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     25/100      4.82G      0.476      0.315     0.9518         52        416: 100%|██████████| 133/133 [00:31<00:00,  4.28it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     26/100      4.81G     0.4619      0.311     0.9479         47        416: 100%|██████████| 133/133 [00:30<00:00,  4.30it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     27/100       4.8G     0.4663     0.3158     0.9521         50        416: 100%|██████████| 133/133 [00:30<00:00,  4.35it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     28/100      4.79G     0.4478     0.3025     0.9401         66        416: 100%|██████████| 133/133 [00:30<00:00,  4.38it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     29/100      4.82G      0.443     0.3024     0.9417         41        416: 100%|██████████| 133/133 [00:30<00:00,  4.37it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     30/100      4.81G     0.4446      0.302     0.9378         49        416: 100%|██████████| 133/133 [00:30<00:00,  4.35it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     31/100       4.8G     0.4451     0.3104     0.9423         54        416: 100%|██████████| 133/133 [00:30<00:00,  4.39it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     32/100      4.79G     0.4354     0.3022     0.9431         47        416: 100%|██████████| 133/133 [00:31<00:00,  4.17it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     33/100      4.82G     0.4342     0.2918     0.9399         39        416: 100%|██████████| 133/133 [00:30<00:00,  4.39it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     34/100      4.81G     0.4419        0.3     0.9394         43        416: 100%|██████████| 133/133 [00:30<00:00,  4.39it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     35/100       4.8G     0.4418     0.2925     0.9424         51        416: 100%|██████████| 133/133 [00:30<00:00,  4.36it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     36/100      4.79G     0.4355     0.2926     0.9391         60        416: 100%|██████████| 133/133 [00:31<00:00,  4.29it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     37/100      4.82G      0.427     0.2894     0.9334         54        416: 100%|██████████| 133/133 [00:31<00:00,  4.25it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     38/100      4.81G     0.4115     0.2792     0.9272         69        416: 100%|██████████| 133/133 [00:30<00:00,  4.37it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     39/100       4.8G     0.4189     0.2817     0.9293         42        416: 100%|██████████| 133/133 [00:30<00:00,  4.39it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     40/100      4.79G     0.4114     0.2811     0.9303         55        416: 100%|██████████| 133/133 [00:30<00:00,  4.36it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     41/100      4.82G     0.4018     0.2691     0.9251         43        416: 100%|██████████| 133/133 [00:30<00:00,  4.39it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     42/100      4.81G     0.3912     0.2658     0.9203         46        416: 100%|██████████| 133/133 [00:30<00:00,  4.38it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     43/100      4.81G     0.3983     0.2633     0.9238         42        416: 100%|██████████| 133/133 [00:30<00:00,  4.30it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     44/100      4.79G     0.3838     0.2557     0.9145         54        416: 100%|██████████| 133/133 [00:31<00:00,  4.28it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     45/100      4.82G     0.3878     0.2586     0.9179         38        416: 100%|██████████| 133/133 [00:30<00:00,  4.35it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     46/100      4.81G     0.3813     0.2544     0.9169         59        416: 100%|██████████| 133/133 [00:30<00:00,  4.32it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     47/100       4.8G      0.381     0.2521     0.9176         62        416: 100%|██████████| 133/133 [00:30<00:00,  4.35it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     48/100      4.79G     0.3751     0.2474     0.9143         48        416: 100%|██████████| 133/133 [00:31<00:00,  4.22it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     49/100      4.82G     0.3757     0.2471     0.9155         54        416: 100%|██████████| 133/133 [00:30<00:00,  4.34it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     50/100      4.81G      0.371     0.2447     0.9147         57        416: 100%|██████████| 133/133 [00:30<00:00,  4.34it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     51/100       4.8G     0.3826     0.2483     0.9142         49        416: 100%|██████████| 133/133 [00:30<00:00,  4.30it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     52/100      4.79G     0.3756     0.2515     0.9146         42        416: 100%|██████████| 133/133 [00:30<00:00,  4.35it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     53/100      4.82G     0.3663     0.2439     0.9103         50        416: 100%|██████████| 133/133 [00:30<00:00,  4.31it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     54/100      4.81G     0.3617     0.2401     0.9094         54        416: 100%|██████████| 133/133 [00:30<00:00,  4.35it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     55/100       4.8G     0.3601     0.2386     0.9085         65        416: 100%|██████████| 133/133 [00:30<00:00,  4.37it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     56/100      4.79G     0.3589     0.2341     0.9063         56        416: 100%|██████████| 133/133 [00:30<00:00,  4.38it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     57/100      4.82G     0.3488     0.2275     0.9044         47        416: 100%|██████████| 133/133 [00:31<00:00,  4.26it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     58/100      4.81G     0.3463      0.228     0.9045         34        416: 100%|██████████| 133/133 [00:30<00:00,  4.32it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     59/100       4.8G       0.35     0.2322     0.9085         54        416: 100%|██████████| 133/133 [00:30<00:00,  4.30it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     60/100      4.79G      0.345      0.225     0.9026         55        416: 100%|██████████| 133/133 [00:30<00:00,  4.31it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     61/100      4.82G      0.343      0.226     0.9042         47        416: 100%|██████████| 133/133 [00:30<00:00,  4.29it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     62/100      4.81G     0.3381     0.2204      0.903         58        416: 100%|██████████| 133/133 [00:30<00:00,  4.36it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     63/100       4.8G     0.3357     0.2207      0.904         52        416: 100%|██████████| 133/133 [00:30<00:00,  4.30it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     64/100      4.79G     0.3304      0.214     0.8991         40        416: 100%|██████████| 133/133 [00:31<00:00,  4.25it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     65/100      4.82G     0.3284     0.2162     0.8983         34        416: 100%|██████████| 133/133 [00:30<00:00,  4.32it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     66/100      4.81G     0.3283     0.2135     0.9002         62        416: 100%|██████████| 133/133 [00:31<00:00,  4.26it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     67/100       4.8G     0.3218     0.2126     0.8934         56        416: 100%|██████████| 133/133 [00:30<00:00,  4.36it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     68/100      4.79G     0.3161     0.2064     0.8955         51        416: 100%|██████████| 133/133 [00:31<00:00,  4.19it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     69/100      4.82G     0.3164     0.2064     0.8916         52        416: 100%|██████████| 133/133 [00:31<00:00,  4.28it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     70/100      4.81G     0.3132     0.2027      0.897         45        416: 100%|██████████| 133/133 [00:31<00:00,  4.22it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     71/100       4.8G     0.3098     0.2018     0.8958         43        416: 100%|██████████| 133/133 [00:32<00:00,  4.16it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     72/100      4.79G     0.3066     0.2005     0.8918         58        416: 100%|██████████| 133/133 [00:32<00:00,  4.10it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     73/100      4.82G     0.3055     0.1993     0.8904         73        416: 100%|██████████| 133/133 [00:31<00:00,  4.18it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     74/100      4.81G      0.305     0.2003     0.8942         58        416: 100%|██████████| 133/133 [00:31<00:00,  4.16it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     75/100       4.8G     0.3056     0.2012     0.8937         48        416: 100%|██████████| 133/133 [00:32<00:00,  4.16it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     76/100      4.79G     0.2992      0.197     0.8889         53        416: 100%|██████████| 133/133 [00:31<00:00,  4.17it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     77/100      4.82G     0.2927     0.1893     0.8876         36        416: 100%|██████████| 133/133 [00:31<00:00,  4.17it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     78/100      4.81G     0.2906     0.1911     0.8879         45        416: 100%|██████████| 133/133 [00:31<00:00,  4.16it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     79/100       4.8G     0.2884     0.1905     0.8831         47        416: 100%|██████████| 133/133 [00:32<00:00,  4.10it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     80/100      4.79G       0.29     0.1879     0.8874         54        416: 100%|██████████| 133/133 [00:30<00:00,  4.34it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     81/100      4.82G     0.2811     0.1832     0.8803         37        416: 100%|██████████| 133/133 [00:30<00:00,  4.33it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     82/100      4.81G     0.2879     0.1884     0.8853         43        416: 100%|██████████| 133/133 [00:30<00:00,  4.36it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     83/100       4.8G      0.281     0.1811     0.8838         43        416: 100%|██████████| 133/133 [00:30<00:00,  4.32it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     84/100      4.79G     0.2772     0.1796     0.8819         47        416: 100%|██████████| 133/133 [00:31<00:00,  4.29it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     85/100      4.82G       0.28     0.1831     0.8889         49        416: 100%|██████████| 133/133 [00:31<00:00,  4.25it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     86/100      4.81G     0.2766     0.1791     0.8823         77        416: 100%|██████████| 133/133 [00:31<00:00,  4.21it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     87/100       4.8G     0.2736     0.1775     0.8794         46        416: 100%|██████████| 133/133 [00:31<00:00,  4.17it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     88/100      4.79G     0.2724     0.1743     0.8834         45        416: 100%|██████████| 133/133 [00:31<00:00,  4.22it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     89/100      4.82G     0.2685     0.1729     0.8801         50        416: 100%|██████████| 133/133 [00:31<00:00,  4.26it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     90/100      4.81G     0.2677     0.1718     0.8776         50        416: 100%|██████████| 133/133 [00:31<00:00,  4.23it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     91/100       4.8G     0.2677     0.1719     0.8798         56        416: 100%|██████████| 133/133 [00:31<00:00,  4.27it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     92/100      4.79G     0.2665     0.1702     0.8768         47        416: 100%|██████████| 133/133 [00:30<00:00,  4.34it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     93/100      4.82G     0.2639     0.1682     0.8794         52        416: 100%|██████████| 133/133 [00:30<00:00,  4.30it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     94/100      4.81G     0.2638     0.1721     0.8756         55        416: 100%|██████████| 133/133 [00:32<00:00,  4.11it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     95/100       4.8G     0.2659      0.172     0.8791         46        416: 100%|██████████| 133/133 [00:32<00:00,  4.11it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     96/100      4.79G     0.2604     0.1691     0.8798         45        416: 100%|██████████| 133/133 [00:32<00:00,  4.11it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     97/100      4.82G      0.258     0.1677     0.8774         54        416: 100%|██████████| 133/133 [00:31<00:00,  4.25it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     98/100      4.81G     0.2586     0.1671     0.8797         43        416: 100%|██████████| 133/133 [00:31<00:00,  4.27it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     99/100       4.8G     0.2618     0.1704      0.878         52        416: 100%|██████████| 133/133 [00:30<00:00,  4.31it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "    100/100      4.79G     0.2618     0.1713     0.8809         54        416: 100%|██████████| 133/133 [00:31<00:00,  4.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:03<00:00,  2.14it/s]\n",
      "                   all        248        493      0.994      0.986      0.994      0.947\n",
      "\n",
      "100 epochs completed in 0.930 hours.\n",
      "Optimizer stripped from yolov8/schenk8l/weights/last.pt, 87.6MB\n",
      "Optimizer stripped from yolov8/schenk8l/weights/best.pt, 87.6MB\n",
      "\n",
      "Validating yolov8/schenk8l/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.116 🚀 Python-3.9.16 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 7973MiB)\n",
      "Model summary (fused): 268 layers, 43608150 parameters, 0 gradients, 164.8 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:03<00:00,  2.40it/s]\n",
      "                   all        248        493      0.994      0.986      0.994      0.947\n",
      "Speed: 0.3ms preprocess, 5.6ms inference, 0.0ms loss, 3.6ms postprocess per image\n",
      "Results saved to \u001b[1myolov8/schenk8l\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(model=model, task=task)\n",
    "model.train(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transferred 595/595 items from pretrained weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ultralytics.yolo.engine.model.YOLO at 0x7f98786a47c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load(os.path.join(project_dir, 'weights', 'best.pt'))  # load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.116 🚀 Python-3.9.16 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 7973MiB)\n",
      "Model summary (fused): 268 layers, 43608150 parameters, 0 gradients, 164.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/thjo/Datasets/BolidenSchenk/val.cache... 248 images, 1 backgrounds, 0 corrupt: 100%|██████████| 248/248 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 16/16 [00:03<00:00,  5.22it/s]\n",
      "                   all        248        493      0.994      0.986      0.994      0.949\n",
      "Speed: 0.1ms preprocess, 8.0ms inference, 0.0ms loss, 0.8ms postprocess per image\n"
     ]
    }
   ],
   "source": [
    "val_results = model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.116 🚀 Python-3.9.16 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 7973MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov8/schenk8l/weights/best.pt with input shape (16, 3, 416, 416) BCHW and output shape(s) (16, 6, 3549) (83.5 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.0 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m simplifying with onnxsim 0.4.31...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 11.7s, saved as yolov8/schenk8l/weights/best.onnx (166.5 MB)\n",
      "\n",
      "Export complete (12.0s)\n",
      "Results saved to \u001b[1m/home/thjo/Code/Adopticum/YoloV8/yolov8/schenk8l/weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov8/schenk8l/weights/best.onnx imgsz=416 \n",
      "Validate:        yolo val task=detect model=yolov8/schenk8l/weights/best.onnx imgsz=416 data=/home/thjo/Datasets/BolidenSchenk/data.yaml \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolov8/schenk8l/weights/best.onnx'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.export(format='onnx', dynamic=True, simplify=True, opset=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bolidenV8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
