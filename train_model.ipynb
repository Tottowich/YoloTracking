{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from tools.yolo_utils import read_yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Tasks Supported by YOLOv8:\n",
    "\n",
    "1. **Detection**:\n",
    "   - Objective: Detect objects in an image or video frame and draw bounding boxes around them.\n",
    "   - Use Case: Object detection in various applications such as surveillance, autonomous vehicles, and robotics.\n",
    "\n",
    "2. **Segmentation**:\n",
    "   - Objective: Segment an image into different regions based on its content and assign labels to each region.\n",
    "   - Use Case: Image segmentation, medical imaging, and understanding object boundaries.\n",
    "\n",
    "3. **Classification**:\n",
    "   - Objective: Classify an image into different categories based on its content.\n",
    "   - Use Case: Image classification in various applications such as object recognition, content filtering, and visual search.\n",
    "\n",
    "4. **Pose/Keypoint Detection**:\n",
    "   - Objective: Detect specific points (keypoints) in an image or video frame for tracking movement or pose estimation.\n",
    "   - Use Case: Human pose estimation, motion tracking, and gesture recognition.\n",
    "\n",
    "Note: YOLOv8 utilizes different architectures (e.g., U-Net, EfficientNet) to perform segmentation, classification, and pose/keypoint detection, ensuring accuracy and speed in these tasks.\n",
    "### Models Supported by YOLOv8:\n",
    "The following models are available in YOLOv8 and are suitable starting points for various tasks:\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "| Model Type   | Pre-trained Weights                            | Task                |\n",
    "|--------------|------------------------------------------------|---------------------|\n",
    "| YOLOv8       | yolov8n.pt, yolov8s.pt, yolov8m.pt, yolov8l.pt, yolov8x.pt       | Detection           |\n",
    "| YOLOv8-seg   | yolov8n-seg.pt, yolov8s-seg.pt, yolov8m-seg.pt, yolov8l-seg.pt, yolov8x-seg.pt   | Instance Segmentation |\n",
    "| YOLOv8-pose  | yolov8n-pose.pt, yolov8s-pose.pt, yolov8m-pose.pt, yolov8l-pose.pt, yolov8x-pose.pt, yolov8x-pose-p6 | Pose/Keypoints      |\n",
    "| YOLOv8-cls   | yolov8n-cls.pt, yolov8s-cls.pt, yolov8m-cls.pt, yolov8l-cls.pt, yolov8x-cls.pt     | Classification      |\n",
    "\n",
    "</div>\n",
    "\n",
    "## Model and project settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model    = 'yolov8l.pt'                                    # Initial weights path. Can aslo be a .yaml defining a model, e.g. yolov8m.yaml\n",
    "task     = 'detect'                                        # 'detect', 'classify', 'segment'\n",
    "project  = 'yolov8/'                                       # Project folder\n",
    "name     = 'schenk8l'                                      # Model name\n",
    "exist_ok = True                                            # Overwrite existing project folder\n",
    "seed     = 0                                               # Seed for training\n",
    "resume   = False                                           # Resume training from last.pt from project + name\n",
    "verbose  = False                                           # Print detailed results\n",
    "plots    = False                                           # Plot training results\n",
    "project_dir = os.path.join(os.getcwd(), project, name)     # Project directory\n",
    "device   = '0'                                             # CUDA device, i.e. '0' or '0,1,2,3' or 'cpu'\n",
    "\n",
    "project_settings = {\n",
    "    'name': name,\n",
    "    'project': project,\n",
    "    'exist_ok': exist_ok,\n",
    "    'seed': seed,\n",
    "    'verbose': verbose,\n",
    "    'resume': resume,\n",
    "    'plots': plots,\n",
    "    'device': device,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Datasets\n",
    "These datasets may be used to train or pre-train a model in YOLOv8. The datasets are can also be found [here](https://docs.ultralytics.com/datasets/).\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Detection\n",
    "</summary>\n",
    "\n",
    "| Dataset       | Description                                                      |\n",
    "|---------------|------------------------------------------------------------------|\n",
    "| Argoverse     | 3D tracking and motion forecasting data from urban environments. |\n",
    "| COCO          | Large-scale dataset for object detection, segmentation, and captioning. |\n",
    "| COCO8         | Subset of COCO train and COCO val for quick tests.               |\n",
    "| Global Wheat 2020 | Dataset of wheat head images for object detection and localization. |\n",
    "| Objects365    | High-quality dataset for object detection with 365 categories.   |\n",
    "| SKU-110K      | Dense object detection dataset in retail environments.           |\n",
    "| VisDrone      | Dataset with object detection and multi-object tracking from drone-captured imagery. |\n",
    "| VOC           | Pascal Visual Object Classes dataset for object detection and segmentation. |\n",
    "| xView         | Dataset for object detection in overhead imagery.                |\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Instance Segmentation\n",
    "</summary>\n",
    "\n",
    "| Dataset       | Description                                                      |\n",
    "|---------------|------------------------------------------------------------------|\n",
    "| COCO          | Large-scale dataset for object detection, segmentation, and captioning. |\n",
    "| COCO8-seg     | Subset of COCO with segmentation annotations.                    |\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Pose Estimation\n",
    "</summary>\n",
    "\n",
    "| Dataset       | Description                                                      |\n",
    "|---------------|------------------------------------------------------------------|\n",
    "| COCO          | Large-scale dataset with human pose annotations.                  |\n",
    "| COCO8-pose    | Subset of COCO with human pose annotations.                       |\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Classification\n",
    "</summary>\n",
    "\n",
    "| Dataset       | Description                                                      |\n",
    "|---------------|------------------------------------------------------------------|\n",
    "| Caltech 101   | Dataset with images of 101 object categories for classification.  |\n",
    "| Caltech 256   | Extended version of Caltech 101 with 256 categories.             |\n",
    "| CIFAR-10      | Dataset of color images in 10 classes.                            |\n",
    "| CIFAR-100     | Extended version of CIFAR-10 with 100 categories.                 |\n",
    "| Fashion-MNIST | Dataset with grayscale images of fashion categories.              |\n",
    "| ImageNet      | Large-scale dataset for object detection and classification.      |\n",
    "| ImageNet-10   | Subset of ImageNet with 10 categories.                            |\n",
    "| Imagewoof     | Challenging subset of ImageNet with 10 dog breed categories.      |\n",
    "| Imagenette    | Smaller subset of ImageNet with 10 easily distinguishable classes.|\n",
    "| MNIST         | Dataset of grayscale images of handwritten digits.                |\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Multi-Object Tracking\n",
    "</summary>\n",
    "\n",
    "| Dataset       | Description                                                      |\n",
    "|---------------|------------------------------------------------------------------|\n",
    "| Argoverse     | 3D tracking and motion forecasting data from urban environments. |\n",
    "| VisDrone      | Dataset with object detection and multi-object tracking from drone-captured imagery. |\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Constructing a YOLO Custom Dataset\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Step-by-step\n",
    "</summary>\n",
    "\n",
    "1. Collect and Organize Your Data:\n",
    "   - Gather images relevant to the object(s) you want to detect.\n",
    "   - Create separate directories for images and annotation files.\n",
    "\n",
    "2. Annotate the Images:\n",
    "   - Use an annotation tool to mark the bounding boxes around objects of interest.\n",
    "   - Save annotations in a YOLO-compatible format.\n",
    "   - Text files with one row per object instance: ```<class_id> <x_center> <y_center> <width> <height>```\n",
    "   - This can be done using various tools such as [Roboflow](https://roboflow.com/) **online** or [YoloLabel](https://github.com/developer0hye/Yolo_Label) **offline**.\n",
    "\n",
    "3. Split the Dataset:\n",
    "   - Divide your dataset into training, validation, and testing sets.\n",
    "   - Ensure each set has a representative distribution of classes and object instances.\n",
    "\n",
    "4. Generate data.yaml file:\n",
    "   - Contains information about the dataset and paths to the training, validation, and testing sets.\n",
    "   - Class names are listed in the order of their IDs in ```names```.\n",
    "   - Number of classes is specified in the ```nc``` field.\n",
    "   - Example:\n",
    "   ```yaml\n",
    "      names:\n",
    "      - '0'\n",
    "      - '1'\n",
    "      - '2'\n",
    "      - '3'\n",
    "      - '4'\n",
    "      - '5'\n",
    "      - '6'\n",
    "      - '7'\n",
    "      - '8'\n",
    "      - '9'\n",
    "      nc: 10\n",
    "      path: /path/to/dataset/ # Path to dataset directory.\n",
    "      test: test              # Relative to path above.\n",
    "      train: train            \n",
    "      val: val               \n",
    "   ```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data settings\n",
    "```python\n",
    "data_path  = '/path/to/dataset/data.yaml'   # Path to data.yaml\n",
    "imgsz      = 416                            # Image size.\n",
    "batch_size = -1                             # Batch size, '-1' uses the largest batch size that fits on the GPS(s)     \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = '/home/thjo/Datasets/objects/data.yaml'      # Path to data.yaml\n",
    "imgsz      = 416                                           # Image size.\n",
    "batch      = 16                                            # Batch size, '-1' uses the largest batch size that fits on the GPU(s)\n",
    "fraction   = 1.0                                           # Fraction of dataset to use for training (0-1) Useful for debugging to validate code and convergence\n",
    "\n",
    "data_settings ={\n",
    "    'data': data,\n",
    "    'imgsz': imgsz,\n",
    "    'batch': batch,\n",
    "    'fraction': fraction\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training settings\n",
    "The following are a subset of the available training settings. For a complete list, see [here](https://docs.ultralytics.com/modes/train/#arguments).\n",
    "\n",
    "<details>\n",
    "<summary>lr0 - Initial Learning Rate</summary>\n",
    "The lr0 parameter represents the initial learning rate of the model. It determines the step size at the beginning of training, influencing how quickly the model learns from the data. A higher learning rate can lead to faster convergence, but it may also cause instability or overshooting. Conversely, a lower learning rate may result in slower convergence but can lead to more accurate and stable training.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>val - Validation during Training</summary>\n",
    "The val parameter is a boolean flag indicating whether to validate the model on the validation set during training. Validation allows monitoring the model's performance on unseen data and helps in detecting overfitting or underfitting. By evaluating the model's performance on the validation set, you can make informed decisions regarding model selection and hyperparameter tuning.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>epochs - Number of Epochs</summary>\n",
    "The epochs parameter determines the total number of times the model will iterate over the entire training dataset. An epoch represents a complete pass through the entire training data, and each epoch updates the model's parameters based on the optimization algorithm used. Choosing an appropriate number of epochs is important to balance training time and model convergence. Too few epochs may result in an undertrained model, while too many epochs may lead to overfitting.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>patience - Early Stopping</summary>\n",
    "The patience parameter refers to the number of epochs to wait before stopping training if there is no improvement in the validation metric. It enables early stopping, a technique used to prevent overfitting and improve efficiency. If the model's performance on the validation set does not improve for a specified number of epochs (defined by patience), training is stopped early to avoid wasting computational resources on a non-improving model.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>save_period - Model Saving</summary>\n",
    "The save_period parameter determines how frequently the model is saved during training. Setting a positive value for save_period means that the model will be saved every n epochs. This allows you to have checkpoints of the model at regular intervals during training. Alternatively, setting save_period to -1 disables automatic saving of the model. Manually saving the model can be done at any desired point in the code.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>device - Hardware Device</summary>\n",
    "The device parameter specifies the hardware device to use for training the model. If you have a compatible GPU, you can specify the CUDA device ID (e.g., '0') to utilize GPU acceleration, which can significantly speed up the training process. Alternatively, you can set device to 'cpu' for CPU training. Choosing the appropriate device depends on the availability of hardware resources and the size of the dataset.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>cache - Image Caching</summary>\n",
    "The cache parameter determines whether to cache images for faster training. Caching preprocessed images in memory can improve the training speed by reducing disk I/O and preprocessing overhead. However, caching can consume a significant amount of memory, so it is advisable to consider the available memory resources and the size of the dataset before enabling this option.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>workers - CPU Workers</summary>\n",
    "The workers parameter specifies the number of CPU workers to use for data loading during training. Increasing the number of workers enables parallel data loading, which can speed up the training process, especially when there are bottlenecks in data loading and preprocessing. However, the optimal number of workers depends on the available CPU resources and the complexity of data loading operations.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>optimizer - Optimization Algorithm</summary>\n",
    "The optimizer parameter determines the algorithm used to update the model's parameters during training. Options include SGD (Stochastic Gradient Descent), Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, or 'auto' to automatically select an optimizer based on the model architecture and problem. Each optimizer has its own set of hyperparameters that control the learning process. Choosing an appropriate optimizer and its hyperparameters is crucial for achieving good training performance.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>cos_lr - Cosine Learning Rate Scheduler</summary>\n",
    "The cos_lr parameter indicates whether to use a cosine learning rate scheduler. The cosine learning rate scheduler gradually reduces the learning rate during training. \n",
    "This technique is based on the cosine function and can help the model converge to a better optima. Using a cosine learning rate scheduler can potentially improve the model's performance and generalization.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr0          = 0.005                                      # Initial learning rate\n",
    "val          = False                                      # Validate on validation set during training\n",
    "epochs       = 100                                        # Number of epochs\n",
    "patience     = 25                                         # Stop training after this many epochs without improvement (early stopping)\n",
    "save_period  = -1                                         # Save model every n epochs, -1 to disable\n",
    "device       = '0'                                        # Device (cuda device id)\n",
    "cache        = False                                      # Cache images for faster training\n",
    "workers      = 8                                          # Number of CPU workers\n",
    "optimizer    = 'auto'                                     # Optimizer (SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto)\n",
    "cos_lr       = True                                       # Use cosine learning rate scheduler\n",
    "lrf          = 0.0001                                     # Final learning rate (for cosine scheduler)\n",
    "momentum     = 0.937                                      # SGD momentum/Adam beta1\n",
    "weight_decay = 0.0005                                     # optimizer weight decay\n",
    "warmup_epochs= 3                                          # Warmup epochs (fractions ok)\n",
    "warmup_momentum = 0.8                                     # Warmup initial momentum\n",
    "\n",
    "training_settings = {\n",
    "    'lr0': lr0,\n",
    "    'val': val,\n",
    "    'epochs': epochs,\n",
    "    'patience': patience,\n",
    "    'save_period': save_period,\n",
    "    'device': device,\n",
    "    'cache': cache,\n",
    "    'workers': workers,\n",
    "    'optimizer': optimizer,\n",
    "    'cos_lr': cos_lr,\n",
    "    'lrf': lrf,\n",
    "    'momentum': momentum,\n",
    "    'weight_decay': weight_decay,\n",
    "    'warmup_epochs': warmup_epochs,\n",
    "    'warmup_momentum': warmup_momentum\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations\n",
    "YoloV8 supports a wide range of augmentations. For a complete list, see [here](https://docs.ultralytics.com/usage/cfg/#augmentation).\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "\n",
    "| Parameter    | Value  | Description                                 |\n",
    "|--------------|--------|---------------------------------------------|\n",
    "| hsv_h        | 0.015  | Image HSV-Hue augmentation (fraction)        |\n",
    "| hsv_s        | 0.7    | Image HSV-Saturation augmentation (fraction) |\n",
    "| hsv_v        | 0.4    | Image HSV-Value augmentation (fraction)      |\n",
    "| degrees      | 0.0    | Image rotation (+/- deg)                     |\n",
    "| translate    | 0.1    | Image translation (+/- fraction)             |\n",
    "| scale        | 0.5    | Image scale (+/- gain)                       |\n",
    "| shear        | 0.0    | Image shear (+/- deg)                        |\n",
    "| perspective  | 0.0    | Image perspective (+/- fraction)             |\n",
    "| flipud       | 0.0    | Image flip up-down (probability)             |\n",
    "| fliplr       | 0.5    | Image flip left-right (probability)          |\n",
    "| mosaic       | 1.0    | Image mosaic (probability)                   |\n",
    "| mixup        | 0.0    | Image mixup (probability)                    |\n",
    "| copy_paste   | 0.0    | Segment copy-paste (probability)             |\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "### Standard Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = {\n",
    "    'hsv_h': 0.015,\n",
    "    'hsv_s': 0.7,\n",
    "    'hsv_v': 0.4,\n",
    "    'degrees': 10.0,\n",
    "    'translate': 0.1,\n",
    "    'scale': 0.5,\n",
    "    'shear': 0.0,\n",
    "    'perspective': 0.0,\n",
    "    'flipud': 0.0,\n",
    "    'fliplr': 0.0,\n",
    "    'mosaic': 1.0,\n",
    "    'mixup': 0.0,\n",
    "    'copy_paste': 0.0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    **project_settings,\n",
    "    **data_settings,\n",
    "    **training_settings,\n",
    "    **augmentations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(model=model, task=task)\n",
    "model.train(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load(os.path.join(project_dir, 'weights', 'best.pt'))  # load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_results = model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(format='onnx', dynamic=True, simplify=True, opset=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bolidenV8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
